% ================
% Landon Buell
% Prof. Yu
% CNN Metrics
% 27 January 2020
% ================

\documentclass[12pt,letterpaper]{article}

\usepackage{graphicx}
\usepackage{multicol}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}

% ================================================================

\begin{document}

% ================================================================

\title{An Introduction to Convolutional Neural Network Benchmarks and Metrics}
\author{Landon Buell}
\date{27 January 2020}
\maketitle

% ================================================================

\section{Introduction}
\paragraph*{}Tom Mitchell, former Chair of Machine Learning Department at Carnegie Melon University, describes \textit{machine learning} as:
\begin{quote}
A computer program is said to learn from experience $E$ with respect to some task $T$, and some performance measure $P$, id it's performance on $T$, as measured by $P$, improves with experience, $E$. \cite{Geron}
\end{quote}
\paragraph*{}As the complexity of machine learning algorithms increase, it's important that we can still measure the performance, $P$, as described by Mitchell. In terms of programs structures like Convolutional Neural Networks (CNNs), there exists various sets of benchmarks that we can use to evaluate the performance of that network. Each metric has a particular use and validity and each one may only apply to particular type of algorithm. In this introduction, I will break up mthese benchmarks into two very board categories: \textit{Regression Metrics} and \textit{Classification Metrics}.
\paragraph*{}For this work I will also use a notation convention as used by James Gareth in his book "An Introduction to Statistical Learning". A value or function that is approximated or estimated with  set of data $X$, is denoted with a \textit{hat}. Thus an approximated function, $f$ becomes $\hat{f}$ which produces an estimated output $\hat{Y}$, which can be compared to known outputs, $Y$. Additionally, $X$ and $Y$ can be treated as discrete vector-like objects, which can be indexed through with a subscript. Thus sets $X$ and $Y$, which $n$ elements are equivalently:
\begin{equation}
X = [x_1, x_2 , x_3 , ... , x_i , ... , x_{n-1} , x_n]
\end{equation}
and
\begin{equation}
Y = [y_1, y_2 , y_3 , ... , y_i , ... , y_{n-1} , y_n]
\end{equation}

% ================================================================

\section{Regression Metrics}
\paragraph*{}Generally, a regression algorithm is a type of Machine Learning Algorithm that seeks to build a function $f$, using a set of data $X$ such that the result of the function is continuous, real number value \cite{Petrik}. In function notation:
\begin{equation}
\label{regression function}
f: X \rightarrow \mathbb{R}
\end{equation}
A regression algorithm generally takes some function and applies a \textit{best fit} line or curve through the data, $X$ as a method of approximation \cite{Gareth}. The output of that function is $Y \in \mathbb{R}$. 
\paragraph*{}The estimated function $\hat{f}$  operates as:
\begin{equation}
\hat{f}(X) = \hat{Y}
\end{equation}
or for each individual element of $X$:
\begin{equation}
\hat{f}(x_i) = \hat{y}_i
\end{equation}

% ================================

\subsection{Mean Squared Error}
\paragraph*{}Mean squared error (MSE) is a very basic way of quantifying how well the predicted response matches a known set of responses. To compute this value, we take the known output for a sample, $y_i$ and find it's difference from the predicted output $\hat{f}(x_i)$, and then the result is squared. This quantity is repeated for each index, $i$, and divided by the total number of elements in the set, $n$.
\begin{equation}
\label{MSE}
MSE = \frac{1}{n}\sum_{i=1}^{n} \Big( y_i - \hat{f}(x_i) \Big)^2
\end{equation}
\paragraph*{}So if the predicted response from $\hat{f}$ differs very slightly from each index in the expected response, $y_i$, then the whole MSE takes on a very small values. This indicates a regression that fits the data well. Conversely, if the predicted response differs greatly from each index, the the whole MSE error takes on a very large value, and indicates that the data is not fit very well. In general, the goal of regression algorithms is to minimize the MSE value \cite{Petrik}.

% ================================

\subsection{Root Mean Squared Error}


% ================================

\subsection{Variance and Standard Deviation}

% ================================================================

\section{Classification Metrics}
\paragraph*{}A classification algorithm is a type of Machine Learning Algorithm that seeks to build a function $f$, using a set of data $X$, such that the result of the function is one of $k$ classes \cite{Petrik}. In function notion:
\begin{equation}
\label{classification function}
f: X \rightarrow \{1,2,...,k-1,k\}
\end{equation}
A classification algorithm takes some set of data, $X$, and using qualities or \textit{features} of that data, places each element, $x_i$ into a bin, called a \textit{class} with other similar elements \cite{Geron}. 

% ================================

\subsection{K - Folds Cross Validation}
\paragraph*{} K - Folds Cross Validation (also called K-Folds X-val) is a method that is part of a larger set of cross validation algorithms. K- Folds is a resampling procedure that allows for the evaluation of a certain model based on a limited set of data \cite{Brownlee}. For the algorithm to work, it requires that there be a data set $X$, that is fully labeled, and can be broken down into an arbitray amount of subsets.
\paragraph*{}The procedure revolves around the parameter $k$, which gives the algorithm the amount of sub groups that sample data is split into for the duration of it's execution. Very generally, the procedure of the algorithm is as follows:
\begin{enumerate}
\item[1.]Shuffle data set $X$ as needed. Split the data set into $k$ similarly sized subsets. \\$X \rightarrow \big[ X_1 , X_2 , ... X_k \big]$.
\item[2.]For each set of data, $X_n$, save it as a testing subset. Use the remaining $k-1$ subsets as a the set of training data for the model.
\item[3.]Fit the model with $k-1$ subsets and evaluate. Hold the evaluation (can be percentage accuracy) and discard the particular model.
\item[4.]Repeat steps 2 and 3 for the remaining $k-1$ subsets.
\end{enumerate}
\paragraph*{}After the procedure has been completed for all $k$ subsets, the results can be amalgamated as desired. This method ensures that every sample in the entire original data set, $X$ is used at least once as a testing element, and $k-1$ times as a training element. The exact value of $k$ is contingent on the particular data set. No single value is useful for all types of models. Often, $k$ should be chosen such that each subset is also statistically similar to the larger data set \cite{Geron}. Additionally, the value $k = 10$ has been experimentally shown to produce generally low-biased estimates on the performance of a classifier \cite{Brownlee}.
\paragraph*{}it is important to recognize that percent accuracy alone is a very poor way of determining the validity of model \cite{Geron}. K- Folds alone provides very little valuable information, and thus in often used in conjunction with other benchmark methods.

% ================================

\subsection{Confusion Matrix}
\paragraph*{}A Confusion Matrix is a one of the most standardized tools in evaluating the performance of a classifier program. Suppose a classifier program has been constructed to place input samples into one of $k$ classes. Thus, the confusion matrix, $C$, associated with that algorithm takes the form of a $k \times k$ matrix. The columns of that matrix represent predicted classes, and the rows represent actual classes \cite{Geron}. A confusion matrix is generally assembled from the results of some sort of K-Folds Cross Validation Algorithm. 
\paragraph*{}Each element of this matrix, $C_{i,j}$ indicated the number of samples predicted by the classifier to be in class $k_i$, but are actually labeled as being in class $k_j$. In the case where $i = j$ (the main diagonal entries), that indicates samples that were correctly predicted to be in the class that they are labeled as. This means that a confusion matrix with strong main diagonal entries indicates a classifier program that performs very well. 
\paragraph*{}In the example case where the entry $C_{a,b}$ has a large value, then that is an indication that several samples are being placed into class. It also may be likely that element $C_{b,a}$ also has a similarly large entry. This means that classes $a$ and $b$ are commonly confused with each other. 
\paragraph*{}In Geron's book, \textit{Hands on Machine Learning with Sci-kit Learn and Tensorflow}, \cite{Geron}, he uses the MNIST data base of 70,000 hand-written and labeled figures to build a 10 - Fold Classifier. In the program that he outlines in Chapter 3, he used a SGD Classifier to classify the images based on the digit $0$ through $9$. For the particular classification algorithm, the confusion matrix, when color coded in gray-scale, looks like:
\begin{figure}[H]
\begin{center}
\textbf{Image of Confusion Matrix Goes Here}
\end{center}
\end{figure}


% ================================

\subsection{Precision Score}
\paragraph*{}The Precision score of a classification algorithm is most commonly applied to a \textit{binary classifier}. Once a cross - validation score algorithm has been implemented to a binary classifier (With outputs \textsf{True} or \textsf{False}), It can be used to create a $2 \times 2$ confusion with the elements \textit{True Positive}, \textit{True Negative}, \textit{False Positive} and \textit{False Negative}, represented as TP,TN,FP,FN respectively \cite{Gareth}. The confusion matrix is then:
\begin{equation}
C = 
\begin{bmatrix}
\text{True Positive} & \text{False Negative} \\
\text{False Positive} & \text{True Negative} \\
\end{bmatrix}
\end{equation}
Using these quantities, we can construct the \textit{precision score} of the classifier:
\begin{equation}
\label{precsision score}
P = \frac{TP}{TP + FP} = \frac{C_{1,1}}{C_{1,1}+C_{2,1}}
\end{equation}
\paragraph*{}The value of $P$ is bounded between $0$ and $1$. In short, the precision score is a measurement of \textit{how many selected items are relevant} to the problem. Notice is deals only with the first column of the matrix $C$. The precision socre is a metric that determines the ratio between how many elements selected were correct (TP) to how many total elements were selected (TP + FP). It is the fraction of correct detection by the model \cite{Goodfellow}. 

% ================================

\subsection{Recall Score}
\paragraph*{}The Recall score of of a classification algorithm is also mostly used with a binary classifier. Again, it begins by implementing some sort of cross - validation algorithm, and then constructing the results into a $2 \times 2$ confusion matrix.:
\begin{equation}
C = 
\begin{bmatrix}
\text{True Positive} & \text{False Negative} \\
\text{False Positive} & \text{True Negative} \\
\end{bmatrix}
\end{equation}
Once again with these, we can compute the \textit{recall score} of the classifier:
\begin{equation}
\label{recall score}
R = \frac{TP}{TP + FN} = \frac{C_{1,1}}{C_{1,1}+C_{1,2}}
\end{equation}
\paragraph*{}Once again, the value of $R$ is bounded between $0$ and $1$. The recall score is a a measurement of \textit{how many relevant items are selected} in the problem. Just as precision deals only with the first column of the $C$ matrix, the recall deals only with the first row of that same matrix. It is the ratio bwteen how many elements selected were correct (TP) to how many elements are labeled as such (TP + FN). It is the fraction of true events that were detected \cite{Goodfellow}.

% ================================

\subsection{F1 Score}
\paragraph*{}Precision and Recall are both valuable metrics of a binary classifier, but each one comes at the cost of the other. Thus a higher precision causes a lower recall and a higher recall causes a lower precision. This concept is called \textit{precision - recall tradeoff} \cite{Geron}. We can actually adjust parameters in our classifier to allow for the favoring of one measure over the other, but this presents a whole new set variables and is certainly dependent on the situation at hand.  The $F1$ (may also be notated as $F_1$) score of a classifier allows for a sort of compromise between $P$ and $R$. It is the harmonic mean between precision and recall \cite{Goodfellow}. It is defined as:
\begin{equation}
\label{F1 score}
F1 = 2\Big(\frac{P \times R}{P + R}\Big)
\end{equation}
\paragraph*{}Once again, the $F1$ score is bounded between $0$ and $1$. It is also the area underneath the curve created by mapping $P$ as a function of $R$ or vice-versa - this is called a $P-R$ curve \cite{Goodfellow}. The $F1$ score is high (closer to $1$) when the algorithm has both a high precision and recall score, and a low $F1$ score when the two differ greatly.



% ================================================================

\begin{thebibliography}{9}
\bibliographystyle{apalike}

\bibitem{Brownlee}
Brownlee, Jason. “A Gentle Introduction to k-Fold Cross-Validation.” \textit{Machine Learning Mastery}, 8 Aug. 2019, machinelearningmastery.com/k-fold-cross-validation/.

\bibitem{Gareth}
James, Gareth, et al. {An Introduction to Statistical Learning with Applications in R}. Springer, 2017.

\bibitem{Geron}
Géron Aurélien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. O'Reilly, 2017.

\bibitem{Goodfellow}
Goodfellow, Ian, et al.\textit{Deep Learning}. MIT Press, 2017.

\bibitem{Petrik}
Petrik, Marek. “Introduction to Machine Learning.” Machine Learning. 22 Jan. 2020, Durham, New Hampshire.

\end{thebibliography}

% ================================================================

\end{document}