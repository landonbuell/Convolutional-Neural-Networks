% ================
% Landon Buell
% Optimizer Notes
% 
% 25 June 2020
% ================

\documentclass[12pt,letterpaper]{article}

\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[top=2.5cm,left=2.5cm,right=2.5cm]{geometry}
\usepackage{fancyhdr}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algpseudocode}

\pagestyle{fancy}
\lhead{Landon Buell}
\rhead{Neural Network Optimizers}
\cfoot{\thepage}


\begin{document}

% ================================================================

\title{Optimization Methods for Deep Feed-Forward Neural Networks}
\author{Landon Buell}
\date{25 June 2020}
\maketitle

% ================================================================

\section{Introduction}


\subsection{Deep Feed-Forward Neural Network Structure}

\paragraph*{}A Deep-Feed forward neural network is a class of architectures built around layers of operations that are called in a fixed sequence. Each layer, $(l)$ takes some input object $x^{(l-1)}$, applies some operation, $f^{(l)}$, and produces some output $x^{(l)}$, where the super-script indicates a layer index. The object $x^{(l)}$ then becomes the input for the next layer of the network \cite{Geron,Goodfellow}. This repeated process can be shown mathematically by:
\begin{equation}
\label{feed-forward}
x^{(l)} = f^{(l)}\big[x^{(l-1)}\big]
\end{equation}
A model with $L$ layers requires an input object $x^{(0)}$, which is then transformed by $L-1$ layers to produce object $x^{(L-1)}$, which we refer to as the output of the network, also denoted by the variable $y^*$

\paragraph*{}Each layer of the network takes the form of a function-like object $f$ that takes an input $x \in \mathbb{R}^N$ and transforms it into an output $y^* \in \mathbb{R}^M$. Typically, $f$ can take the form of a matrix-vector equation, a $k$-dimensional convolution operation, a pooling operation, a non-linear activation function, or any other general transformation. Often, these operations may be combined to produce composite functions in each layer. In each case, there as a series of parameters associated with the function $f$. In the case of a matrix-vector equation, there are the elements inside each matrix and bias vector, and in the case of convolution, there are the weighting values within the kernel itself. 

\paragraph*{}The nature of the deep neural network is a multilayered function composition:
\begin{equation}
\label{DNN Composition}
y^* = x^{(L-1)} = f^{(L-1)}\big[ f^{(L-2)}...f^{(1)}\big[f^{(0)}\big[ x^{(0)} \big]\big]\big]\big]
\end{equation}
Each function is composed of a series of variable parameters, called \textit{weights}, $W$, and \textit{biases}, $b$. Each layer-function may also contain a series of static parameters such as an nonlinear activation function,$a$. In a densely connectivity deep neural network, we define a forward pass for the single layer (\ref{feed-forward}) to take some variation of the form:
\begin{equation}
x^{(l)} = f^{(l)}\big[x^{(l-1)}\big] = a^{(l)} \big[ W^{(l)} x^{(l-1)} + b^{(l)} \big]
\end{equation}
For a network with $L$ layers, we can define a forward pass computation with algorithm (\ref{algFeedForward})

\begin{algorithm}[H]
\caption{Forward propagation system in a standard deep neural network. Each iteration in the main \textit{for-loop} represents the execution of a layer, adn passing the result to the "next" layer function. A practical application of this algorithm should include batches of samples instead of a single sample.}
\label{algFeedForward}

\begin{algorthimic}
\REQUIRE : Network with $L$ layers - $0,1,2,....,L-2,L-1$
\REQUIRE : Set of weighting parameters $W^{(i)} \in \{0,1,....,L-1\}$


\end{algorithmic}
\end{algorithm}

\subsection{Optimization Motivation}

\paragraph*{}Ultimately any output is function of the input, and the parameters in each layer. By convention, these parameters can be concatenated into a single object called $\Theta$. For smaller neural networks, $\Theta$ may only have several hundred elements, but for increasingly complex models, there can upwards of several million elements within the object. The goal of the network then is to find a particular set of elements $\{ \theta_0 , \theta_1 , ... \}$ that allow a network to produce a desirable output based on that input. To do this, we require a set of samples $X = \{x_0,x_1,x_2,....\}$ and a set of corresponding labels 
$Y = \{y_0,y_1,y_2,...\}$, such that element $x_i$ should produce result $y_i$, when passed into the network model. 

\paragraph*{}In reality, a network will not produce output $y_i$ from samples $x_i$ exactly, but will instead produce an \textit{approximation} of $y_i$ which we denote as $y_i^*$. We can use a function $J$ called the \textit{objective function} which compares the expected output, $y_i$, to the actual output, $y_i^*$ and produces a numerical score ranking how much the two values differ from each other. This score is called the \textit{cost} of the sample \cite{James}, lower numbers being favorable, showing a low difference or low cost of the sample.

\paragraph*{}The value of the cost function is then directly dependent on the expected output, $y_i$ and the given output $y_i^*$. The cost of a single sample can be shown as $J = J( y_i , y_i^* )$. However, the output is implicitly dependent on the the network input $x_i$ and the parameters in each layer, given by $\Theta$. Since the latter two are fixed objects, we can then only adjust the elements in the object $\Theta$. We do so to allow for increasingly lower values of the objective function, $J(\Theta)$, which indicates that on average, the network model is producing more and more accurate predictions. This process is called \textit{optimization}. 

\paragraph*{}It is worth noting that machine learning does differ from explicit multi-dimensional optimization. In machine learning, the performance of a neural network is typically measured by a standard statistical metric such as precision score, recall score, accuracy rating, etc. \cite{Geron}. These measures are far more interpretable to a human, and often more indicative that a model is performing as desired than the cost function itself. Thus, we optimize the elements in $J(\Theta)$ in hopes that doing so will also improve these related performance metrics \cite{Goodfellow}.Despite this indirect optimization, it is the basis for much of machine-learning.

\subsection{The $\Theta$ Object}

\paragraph*{}It is worth reinforcing that although we have implied $\Theta$ to be a single dimensional column-vector-like object, it can in reality, hold just about any shape. It may be more convenient to express it as a \textit{set}, containing the parameters for each layer's kernels, weights and biases in the form of vectors or matrices. It is often reasonable to convey $\Theta$ not as a tensor-like object, but rather a set of objects, concatenated from each layer: 
\begin{equation}
\label{THETA}
\Theta = \big\{ W^{(0)} , b^{(0)} , W^{(i)} , b^{(i)}, W^{(l)} , b^{(l)}, ....,
W^{(L-1)} , b^{(L-1)} \big\} 
\end{equation}
Where $W^{(i)}$ , $b^{(i)}$ gives a weighting and element in the $i$-th layer. 

\paragraph*{}Each element  $W^{(1)}$, $b^{(l)}$ is an array-like object which contains further elements (floating-point numbers). For example: 
$W^{(l)} \in \mathbb{R}^{N \times M} = \big\{ \theta_0 , \theta_1 , ... , \theta_{N \times M - 1} \big\}$. Then, $\Theta$ is an object that contains matrices and vectors corresponding to the numerous parameter contained within each layer. Each of those parameters can be further broken down into it's constituent elements which are $\theta_0 , \theta_1 , \theta_2 , .... $. 



% ================================================================

\section{Gradient Based Learning}

\paragraph*{}A large trouble in machine learning and deep learning arises from the size of the dimensionality of $\Theta$. For example, in two immediately adjacent \textit{Dense Layers}, which $n$ units in layer $(l)$ and $n$ units in layer $(l+1)$, there are $(m \times n) + m$ elements added to $\Theta$. Often, deep learning networks can contain dozens of densely connect layers with hundreds of neurons in each layer. This pushes the size of $\Theta$ to grow very quickly. The number of elements in $\Theta$ is the dimensionality of the network's parameter space \cite{Goodfellow}. This space contains all possible combinations of parameters of the networks.

\paragraph*{}We need to then find the appropriate position in parameter-space as to minimize $J$. This task becomes even more difficult when considering that we do not define $J$ explicitly for each position in the space. Furthermore, $J$ may often contain discontinuities, or ill-conditionings, and is not easily represented by a traditional analytical function, thus we cannot solve the optimization with standard whiteboard mathematics. For deep and complex nerual networks, even over a finite set of possible floating-point values, this can amount to trillions of possible locations in parameter space to test, which is unpractical for computational implementation. This prevents us from traditional brute-force testing methods in hopes to find a local or global minimum. 

\paragraph*{}Instead, we brute-force a \textit{guess} or series of guesses for a possible solution. This guess is a particular point in parameter space that either be chosen randomly, or derived from a previously found solution. We can then optimize $J(\Theta)$ based on the guess or collection of guesses by adjusting each value in $\Theta$ as the reduce the value of $J$ by a small quantity. By repeating this process hundreds of times, under the right conditions, we can take a pseudo-random guess at the solution and slowly adjust each element to develop an approximation of the solution. This process is a simplification of \textit{gradient descent} algorithm.

\subsection{Gradient Descent}

\paragraph*{}Gradient Descent aims to compute the numerical gradient of the cost function at some potion in parameter-space and use the negative gradient to update the elements in the space, as to take a \textit{step} to reduce the value of the cost from the previous step. For example, suppose we guess a set of parameters, $\theta^*$, in a $k$-dimensional parameter-space, $\theta^* \in \mathbb{R}^k$. We then compute the gradient of the cost function, with respect to each parameter in $\Theta$, which we denote as $\nabla_\Theta \big[ J(\Theta) \big]$. Recall the that gradient operator, $\nabla$ returns a vector where each element is partial derivative of $J$ with respect to that parameter:

\begin{equation}
\label{cost gradient}
\nabla_\Theta \big[ J(\Theta) \big] = \nabla_\Theta \big[ J(\theta_0, \theta_1,\theta_2,\theta_3,...) \big] =
\Big[ \frac{\partial J}{\partial \theta_0}, \frac{\partial J}{\partial \theta_1}, 
		\frac{\partial J}{\partial \theta_2}, \frac{\partial J}{\partial \theta_3}, ... \Big]^T
\end{equation}

\paragraph*{}Both $\Theta^*$ and $\nabla_\Theta \big[ J(\Theta) \big]$ exist in the same dimensional space, $\mathbb{R}^k$, and thus can be added together - or rather we add the \textit{negative} gradient to decrease the cost. The elements in $\Theta^*$ provides a sort of starting point, and the elements in $-\nabla_\theta \big[ J(\Theta) \big]$ act as a sort of \textit{step} to take, such that changing each element, $\theta^*_i$ by some amount $\frac{\partial J}{\partial \theta_i}$ results in a small decrease in the overall cost function. Generally, across all parameters in the parameters object, we note this update rule as:

\begin{equation}
\label{gradient step}
\Theta^* = \Theta^* + -\alpha \nabla_\Theta \big[ J(\Theta) \big]
\end{equation}

\paragraph*{}Another trouble arises when we reconsider that each element in the gradient vector is often not well-defined by a traditional mathematical function, meaning we cannot simply use symbols and standard calculus rules to find the necessary values in eqn. (\ref{gradient step}), we need to use numerical approximations, and \textit{back propagation} to compute each partial derivative. 

\subsection{Backwards-Propagation}

\paragraph*{}Given the recursive nature of the function, we cannot compute each element in the gradient vector by conventional derivatives. We have to use the chain rule from calculus to  begin at the final output of the network and work backwards to the entry layer to compute each derivative. This process is called \textit{back-propagation}. 

\paragraph*{}To begin, we compute the gradient of the cost function with respect each element in the output layer. $\nabla_{y^*}\big[J(\Theta)\big]$. Next, recall that a layer's operation represented by the function $f^{(l)}$ may often be the composition of many functions, such as a dense layer having a linear matrix-vector product, which may then be subject to a non-linear activation function. We also need to apply the chain rule within each layer to ensure that each element is accurately computed.

\paragraph*{}By Computing the derivative in any given layer $l$, we can use the chain rule to recursively back-ward propagate through the network to ensure that every parameter in the $\Theta$ object is accounted for. Rather than explicitly derive the mathematical rule for a single network type, we outline a general back-propagation procedure for a general feed-forward network. Each layer is presumed to follow a two-step structure:
\begin{equation}
\begin{split}
z^{(l)} &=  W^{(l)} x^{(l-1)} + b^{(l)} \\
x^{(l)} &= a^{(l)} \big[ z^{(l)} \big]
\end{split}
\end{equation}
Where $W^{(l)}$ is a weighting matrix, $b^{(l)}$ is a bias vector, and $x^{(l-1)}$ is the activation from the previous layer. The array $z^{(l)}$ represents the \textit{pre-activations} to which the an activation function $a^{(l)}$ is applied to. This algorithm is adapted from Goodfellow, 2017 \cite{Goodfellow}.

\begin{algorithm}
\label{back-prop}
\caption{Backwards Propagation System in a fully connected neural network model for a single sample.}
\begin{algorithmic}

\REQUIRE Network depth of $L$ layers ,  cost function $J$ , learning rate $\alpha$
\REQUIRE Object $\Theta$ containing all network parameters as in eqn. (\ref{THETA})
\REQUIRE Object of activation function derivatices: $\partial\vec{a}^{(i)}$ 
\REQUIRE Object of final activations by layer: $\vec{x}^{(i)}$ with $i \in [1,2,...,L-2,L-1]$
\REQUIRE Object of pre-activations by layer: $\vec{z}^{(i)}$ with $i \in [1,2,...,L-2,L-1]$
\REQUIRE Sample target output $y$ (usually one-hot-encoded)

Array to hold weight updates: $\nabla W \leftarrow \big\{ \big\}$ \\
Array to hold bias updates: $\nabla b \leftarrow \big\{ \big\}$) \\
Compute gradient of $J$ w.r.t output layer $\vec{x}^{(0)}$: \\

$dx \leftarrow \nabla_{y^*} \big[ J(y^*,y) \big]$ \\

\FOR {$l = L-2 , L-3 , ... ,2,1$}
	\item Compute gradient of pre-activation values: 
	\item $dx \leftarrow \nabla_{z^{(l)}} \big[ J \big] = dx \odot \partial\vec{a}^{(l)}\big[ z^{(l)} \big]$
	\item Compute gradient w.r.t parameters in bais vectors and weighting arrays:
	\item $db^{(l)} \leftarrow \nabla_{b^{(l)}} \big[ J \big] = dx$
	\item $dW^{(l)} \leftarrow \nabla_{W^{(l)}} \big[ J \big] = dx \cdot x^{(l-1)}$
	\item Insert $db^{(l)}$ and $dW^{(l)}$ into arrays $\nabla b$ and $\nabla W$ respectively

\ENDFOR

ASSERT: $dW^{(i)}$ has same dimension as $W^{(i)}$ for $i \in [0,1,...,L-3,L-2]$ \\
ASSERT: $db^{(i)}$ has same dimension as $b^{(i)}$ for $i \in [0,1,...,L-3,L-2]$ \\

\FOR {$l = 0,1,...,L-3,L-2$}
	\item Use each $db$ and $dW$ object, and the learning rate to 
	\item $W^{(i)} \leftarrow W^{(i)} + (\alpha) dW^{(i)}$
	\item $b^{(i)} \leftarrow b^{(i)} + (\alpha) db^{(i)}$
\ENDFOR

\RETURN Object containing updated weights: $\hat{W}^{(i)}$ with $i \in [1,1,...,L-2,L-1]$
\RETURN Object containing updated biases: $\hat{b}^{(i)}$ with $i \in [1,1,...,L-2,L-1]$

\end{algorithmic}
\end{algorithm}




% ================================================================

\section{Stochastic Gradient Descent}

\paragraph*{}Stochastic Gradient Descent (SGD) is considered to be amoung the more powerful and versatile variations of basic gradient-based optimization methods. It follows all of the same rules of previously introduce gradient descent, but add adds are sort of augmentation to it as well.


% ================================================================


% ================================================================

\begin{thebibliography}{9}
\bibliographystyle{apalike}

\bibitem{Geron}
Géron Aurélien. \textit{Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}. O'Reilly, 2017.

\bibitem{Goodfellow}
Goodfellow, Ian, et al.\textit{Deep Learning}. MIT Press, 2017.

\bibitem{James}
James, Gareth, et al. \textit{An Introduction to Statistical Learning with Applications in R}. Springer, 2017.


\end{thebibliography}


% ================================================================

\end{document}
