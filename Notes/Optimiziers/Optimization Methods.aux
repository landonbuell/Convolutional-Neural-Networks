\relax 
\citation{Geron}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Deep Feed-Forward Neural Network Structure}{1}\protected@file@percent }
\newlabel{feed-forward}{{1}{1}}
\newlabel{DNN Composition}{{2}{1}}
\citation{James}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Forward propagation system in a standard deep neural network. Each iteration in the main \textit  {for-loop} represents the execution of a layer, and passing the result to the "next" layer function. A practical application of this algorithm should include batches of samples instead of a single sample.}}{2}\protected@file@percent }
\newlabel{algFeedForward}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Optimization Motivation}{2}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The $\Theta $ Object}{3}\protected@file@percent }
\newlabel{THETA}{{4}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Gradient Based Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gradient Descent}{4}\protected@file@percent }
\newlabel{cost gradient}{{5}{4}}
\citation{Goodfellow}
\newlabel{gradient update}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backwards-Propagation}{5}\protected@file@percent }
\citation{Geron}
\citation{James}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backwards propagation system, in a standard deep neural network. Each iteration in the first \textit  {for-loop} computes the gradient of the cost function $J$ with respect to the weight and bias arrays. Each element in those arrays is then the discrete gradient of that parameter. A practical application of this algorithm should include batches of samples instead of a single sample}}{6}\protected@file@percent }
\newlabel{algBackProp}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Challenges of Gradient-Based Learning}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Stochastic Gradient Descent}{6}\protected@file@percent }
\citation{Goodfellow}
\citation{Geron}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Stochastic Gradient Descent (SGD) in a neural network}}{7}\protected@file@percent }
\newlabel{algSGD}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Momentum Hyper-parameter}{7}\protected@file@percent }
\citation{Geron}
\citation{Goodfellow}
\newlabel{momentum update}{{8}{8}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Stochastic Gradient Descent (SGD) optimizer with momentum}}{8}\protected@file@percent }
\newlabel{algSGD}{{4}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Nesterov's Momentum}{9}\protected@file@percent }
\newlabel{Nesterov update}{{9}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Stochastic Gradient Descent (SGD) optimizer with Nesterov momentum}}{9}\protected@file@percent }
\newlabel{algSGD}{{5}{9}}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\citation{Geron}
\@writefile{toc}{\contentsline {section}{\numberline {4}Adaptive Learning Algorithms}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Adaptive-Gradients}{10}\protected@file@percent }
\newlabel{AdaGrad update}{{10}{10}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Adaptive Gradient (AdaGrad) optimizer for a neural network}}{11}\protected@file@percent }
\newlabel{algAdaGrad}{{6}{11}}
\citation{Geron}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}RMS-Prop}{12}\protected@file@percent }
\newlabel{RMSprop Update}{{11}{12}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces RMS-Propagation (RMSprop) optimizer for a neural network}}{12}\protected@file@percent }
\newlabel{algAdaGrad}{{7}{12}}
\citation{Geron}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Adaptive-Moments}{13}\protected@file@percent }
\newlabel{ADAM update}{{12}{13}}
\bibstyle{apalike}
\bibcite{Geron}{1}
\bibcite{Goodfellow}{2}
\bibcite{James}{3}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Adaptive-Moments (ADAM) optimizer for a neural network}}{14}\protected@file@percent }
\newlabel{algAdaGrad}{{8}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Approximate Second-Order Methods}{14}\protected@file@percent }
