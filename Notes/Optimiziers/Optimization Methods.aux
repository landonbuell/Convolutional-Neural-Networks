\relax 
\citation{Geron}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Deep Feed-Forward Neural Network Structure}{1}\protected@file@percent }
\newlabel{feed-forward}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Optimization Motivation}{1}\protected@file@percent }
\citation{James}
\citation{Geron}
\citation{Goodfellow}
\citation{Goodfellow}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}The $\Theta $ Object}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Gradient Based Learning}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gradient Descent}{3}\protected@file@percent }
\newlabel{cost gradient}{{3}{3}}
\citation{Goodfellow}
\newlabel{gradient step}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Backwards-Propagation}{4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Use backwards-propagation to recursively compute each element in the gradient vector of the cost function $J$ with respect to each network parameter $\theta _i$.}}{4}\protected@file@percent }
\bibstyle{apalike}
\bibcite{Geron}{1}
\bibcite{Goodfellow}{2}
\bibcite{James}{3}
\@writefile{toc}{\contentsline {section}{\numberline {3}Stochastic Gradient Descent}{5}\protected@file@percent }
